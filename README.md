# ğŸ§ Automated Podcast Transcription and Topic Segmentation

> *AI-powered system for transcribing long-form podcast audio and automatically segmenting it into topical sections*

## ğŸ“‹ Project Overview

The goal of this project is to develop an **AI-powered system** that automatically transcribes podcast audio recordings and segments them into distinct topical sections.

Leveraging advances in **speech-to-text technology (ASR)** and **natural language processing (NLP)**, the system enables users to navigate podcasts efficiently by browsing topics, key discussion points, and compact summaries without listening to the entire episode.

### ğŸ¯ Key Outcomes

- ğŸ™ï¸ Understand speech recognition techniques for converting audio to text
- ğŸ§  Implement NLP methods to identify topic changes and segment transcripts
- ğŸ”„ Build an end-to-end pipeline: audio ingestion â†’ preprocessing â†’ transcription â†’ segmentation â†’ indexing
- ğŸ“Š Visualize segment boundaries, extract keywords, and generate compact summaries for each topic
- ğŸ“ Prepare comprehensive documentation and final presentation describing methodology, challenges, and user benefits

### ğŸ—ï¸ Model Architecture

```mermaid
flowchart TD
    A[Podcast Audio Files] --> B[Audio Preprocessing]
    B --> C[Speech-to-Text<br/>OpenAI Whisper]
    C --> D[Transcript Quality<br/>Evaluation]
    D --> E[Topic Segmentation<br/>NLP Algorithms]
    E --> F[Keyword Extraction<br/>TF-IDF]
    F --> G[Summarization<br/>BART Model]
    G --> H[Segmented Transcripts<br/>with Topics & Summaries]
    H --> I[User Interface<br/>Navigation & Search]

```
## ğŸ“Š Dataset

**Chosen Dataset:**

**This American Life Podcast Transcript Dataset** (Kaggle)

- ğŸ”— Link: https://www.kaggle.com/datasets/thedevastator/this-american-life-podcast-transcript-dataset
- ğŸ“ˆ ~600+ episodes
- âœ¨ High-quality aligned transcripts with timestamps and speaker information
- ğŸµ **Matching audio files** legally downloaded from the official archive: https://www.thisamericanlife.org/archive
- ğŸ¯ **Current working subset:** 200 episodes (transcripts + downloaded MP3 audio)

## ğŸ“… Project Milestones & Timeline (6 Weeks - Completed)

| Milestone | Weeks | Notebooks Folder | Main Deliverables | Status |
|-----------|-------|------------------|-------------------|--------|
| 1 | 1â€“2 | `milestone_1/` | Dataset acquisition, exploration, audio preprocessing | âœ… Complete |
| 2 | 3â€“4 | `milestone_2/` | Initial transcription (Whisper), topic segmentation algorithms | âœ… Complete |
| 3 | 5â€“6 | `milestone_3/` | Keyword extraction, summarization, visualizations, system testing & feedback | âœ… Complete |

## ğŸ› ï¸ Technologies Used

- **ğŸ’» Environment**: Python 3.8+ (VS Code + local development)
- **ğŸµ Audio Processing**: librosa, pydub, soundfile, pyloudnorm, noisereduce
- **ğŸ™ï¸ Speech-to-Text**: OpenAI Whisper (tiny/base models)
- **ğŸ§  NLP & Segmentation**: nltk, sentence-transformers, scikit-learn, transformers
- **ğŸ“Š Evaluation**: jiwer (Word Error Rate)
- **ğŸ“ˆ Visualization**: matplotlib, plotly (planned)
- **ğŸŒ UI Framework**: Streamlit (web interface for podcast navigation)

## ğŸš€ Setup Instructions

1. **ğŸ“¥ Clone Repository** (if applicable) or ensure you have the project files

2. **ğŸ Create Python Environment**:
   ```bash
   python -m venv audio_project_env
   # Activate: audio_project_env\Scripts\activate (Windows) or source audio_project_env/bin/activate (Linux/Mac)
   

3. **ğŸ“¦ Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   

4. **âš¡ GPU Setup** (optional, recommended for faster Whisper processing):
   - Install CUDA 11.8+ if you have an NVIDIA GPU
   - PyTorch will automatically detect and use GPU acceleration

5. **âœ… Verify Installation**:
   ```python
   import whisper
   import librosa
   print("Setup complete!")
   

6. **ğŸ“ File Organization**:
   - Raw audio: `data/audio_raw/
   - Processed audio: `data/audio_processed/
   - Transcripts: `data/transcripts_processed/
   - Notebooks: `notebooks/` directory
7. **Run the Web App** (after completing all weeks):
   ```bash
   streamlit run data/app/podcast_navigation_app.py
   
## ğŸ“‚ Project Structure

```
Audio Project/
â”œâ”€â”€ README.md                          # Main project documentation
â”œâ”€â”€ PROJECT_DOCUMENTATION.md           # Detailed week-by-week documentation
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”‚
â”œâ”€â”€ data/                              # Data directory
â”‚   â”œâ”€â”€ README.md                      # Data folder documentation
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ podcast_navigation_app.py  # Main Streamlit web application
â”‚   â”‚   â””â”€â”€ testing_app.py             # Testing app for Week 6
â”‚   â”œâ”€â”€ audio_raw/                     # Original podcast MP3 files (200 episodes)
â”‚   â”œâ”€â”€ audio_processed/               # Preprocessed WAV files (normalized, cleaned)
â”‚   â”œâ”€â”€ audio_tmp/                     # Temporary audio chunks during processing
â”‚   â”œâ”€â”€ transcripts_raw/               # Original reference transcripts
â”‚   â”œâ”€â”€ transcripts_raw_truncated/     # 200-episode subset
â”‚   â”œâ”€â”€ transcripts_processed/         # Whisper-generated transcriptions with timestamps
â”‚   â”œâ”€â”€ episode_images/                # Episode artwork and metadata images
â”‚   â”œâ”€â”€ segmented_outputs/             # Topic-segmented transcripts (JSON format)
â”‚   â”‚   â””â”€â”€ episode_*.json             # Individual episode segments
â”‚   â”œâ”€â”€ segments_processed/            # Processed segment metadata
â”‚   â”‚   â””â”€â”€ all_segments.csv           # Aggregated segment data
â”‚   â””â”€â”€ test/                          # Week 6 testing data
â”‚       â””â”€â”€ segmented_outputs/
â”‚           â””â”€â”€ week6_test/            # Test data for 5 new episodes
â”‚
â””â”€â”€ notebooks/                         # Jupyter notebooks for analysis & development
    â”œâ”€â”€ milestone_1/                   # Foundation & Data Acquisition (Weeks 1-2)
    â”‚   â”œâ”€â”€ week_1/
    â”‚   â”‚   â”œâ”€â”€ project_init_and_dataset_acquisition.ipynb
    â”‚   â”‚   â””â”€â”€ README.md
    â”‚   â””â”€â”€ week_2/
    â”‚       â”œâ”€â”€ audio_preprocessing_and_speech_to_text.ipynb
    â”‚       â”œâ”€â”€ transcript_quality_evaluation.ipynb
    â”‚       â””â”€â”€ README.md
    â”‚
    â”œâ”€â”€ milestone_2/                   # Core Pipeline Development (Weeks 3-4)
    â”‚   â”œâ”€â”€ week_3/
    â”‚   â”‚   â”œâ”€â”€ topic_segmentation_keyword_extraction_summarization.ipynb
    â”‚   â”‚   â””â”€â”€ README.md
    â”‚   â””â”€â”€ week_4/
    â”‚       â”œâ”€â”€ README.md
    â”‚       â””â”€â”€ screenshots/
    â”‚           â”œâ”€â”€ app_theme.png
    â”‚           â”œâ”€â”€ browse_episodes.png
    â”‚           â”œâ”€â”€ main_interface.png
    â”‚           â””â”€â”€ search_topics.png
    â”‚
    â””â”€â”€ milestone_3/                   # Optimization & System Testing (Weeks 5-6)
        â”œâ”€â”€ week_5/
        â”‚   â”œâ”€â”€ README.md
        â”‚   â””â”€â”€ notebooks/             # Optimization experiments
        â””â”€â”€ week_6/
            â”œâ”€â”€ README.md
            â”œâ”€â”€ system_testing.ipynb
            â””â”€â”€ screenshots/
                â”œâ”€â”€ dashboard.png
                â”œâ”€â”€ search_topics.png
                â”œâ”€â”€ test_episodes1.png
                â”œâ”€â”€ test_episodes2.png
                â””â”€â”€ feedback.png
```


## ğŸ“ˆ Current Status

âœ… **Completed:**
- Dataset acquired: 200 episodes (transcripts + audio)
- Environment setup completed (Python environment with GPU support)
- **Week 1**: Project initialization and dataset acquisition
- **Week 2**: Audio preprocessing pipeline and Whisper transcription
- **Week 3**: Topic segmentation, keyword extraction, and summarization
- **Week 4**: Streamlit web application for podcast navigation
- **Week 5**: Advanced processing refinements and pipeline optimization
- **Week 6**: System testing on 5 diverse episodes, structured feedback collection (3â€“5 testers), practical improvement recommendations


ğŸ“‹ **Key Achievements:**
- âœ¨ Full audio preprocessing pipeline (noise reduction, normalization, chunking)
- ğŸ™ï¸ Whisper ASR integration with quality evaluation (WER metrics)
- ğŸ§  Multi-algorithm topic segmentation (TF-IDF, embeddings, LLM-based)
- ğŸ·ï¸ Automated keyword extraction and BART summarization
- ğŸ’¬ Sentiment analysis and topic labeling
- ğŸŒ Interactive Streamlit web app for podcast topic navigation
- ğŸ“Š System testing framework with user feedback collection
- ğŸ“š Comprehensive documentation with visual flowcharts and screenshots


## ğŸ“š References

- Kaggle Dataset: https://www.kaggle.com/datasets/thedevastator/this-american-life-podcast-transcript-dataset
- Audio Source: https://www.thisamericanlife.org (public archive)

---


