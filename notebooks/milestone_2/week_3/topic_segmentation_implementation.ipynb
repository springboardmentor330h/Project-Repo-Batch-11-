{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCzPW4nyi5fN"
      },
      "source": [
        "## TOPIC SEGEMENTATION AND IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MOUNT DRIVE**"
      ],
      "metadata": {
        "id": "nrlITWh37HWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3h88Nw1Q5wvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSTALL DEPENDENCIES**"
      ],
      "metadata": {
        "id": "WTb1Ppe4hwJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers openai transformers python-dotenv nltk scikit-learn pandas tqdm torch"
      ],
      "metadata": {
        "id": "eNrwHnnh0SJp",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1768559173904,
          "user_tz": -330,
          "elapsed": 18569,
          "user": {
            "displayName": "Manasi N",
            "userId": "07680321668724852566"
          }
        }
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJdSW8qOuD50"
      },
      "source": [
        "**IMPORTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ofbmaNR0uHTC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1768559214298,
          "user_tz": -330,
          "elapsed": 30509,
          "user": {
            "displayName": "Manasi N",
            "userId": "07680321668724852566"
          }
        },
        "outputId": "a1bae460-fbc1-41be-85e6-d749d35c7dd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import time   # Used for time-related tasks (like delays or timing code)\n",
        "from tqdm.auto import tqdm    # Shows a progress bar when running loops\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords   # Contains common words like \"the\", \"is\", \"and\"\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Converts text into numerical form using TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Used to measure similarity between texts\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from transformers import pipeline   # Used to run pre-trained NLP models easily\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CHECK IF CUDA IS AVAILABLE**"
      ],
      "metadata": {
        "id": "1BPxqsY531uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available?:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU name:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aogm-rTCjI3v",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1768559219563,
          "user_tz": -330,
          "elapsed": 20,
          "user": {
            "displayName": "Manasi N",
            "userId": "07680321668724852566"
          }
        },
        "outputId": "8a6a6cd7-3354-4a9d-9561-841e01436af0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available?: True\n",
            "GPU name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOAD OpenAI KEY FROM .env FILE**"
      ],
      "metadata": {
        "id": "fwL0dy813vtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv(\"/content/drive/MyDrive/podcast-project/data/.env\")\n",
        "\n",
        "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "\n",
        "if not OPENROUTER_API_KEY:\n",
        "    raise ValueError(\"OPENROUTER_API_KEY not found in .env file\")\n",
        "\n",
        "print(\"OpenRouter API key loaded (hidden)\")\n",
        "\n",
        "# Create client\n",
        "from openai import OpenAI\n",
        "\n",
        "llm_client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=OPENROUTER_API_KEY,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqsMNHaM3-vD",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1768559222483,
          "user_tz": -330,
          "elapsed": 356,
          "user": {
            "displayName": "Manasi N",
            "userId": "07680321668724852566"
          }
        },
        "outputId": "c2439bf7-7127-4214-dd4d-bf00561cfc02"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenRouter API key loaded (hidden)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PATHS**"
      ],
      "metadata": {
        "id": "1ogvUiaklvQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = \"/content/drive/MyDrive/podcast-project\"\n",
        "PROCESSED_DIR = f\"{BASE_PATH}/data/transcripts_processed\"\n",
        "OUTPUT_DIR = f\"{BASE_PATH}/data/segmented_outputs\"\n",
        "COMPARE_DIR = f\"{BASE_PATH}/data/segmented_outputs/algorithm_comparison\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "n8FbzKvvlw5F",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1768559225724,
          "user_tz": -330,
          "elapsed": 38,
          "user": {
            "displayName": "Manasi N",
            "userId": "07680321668724852566"
          }
        }
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONFIGURATION**"
      ],
      "metadata": {
        "id": "n6BACAblvIRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select which episodes to process\n",
        "EPISODE_NUMBERS = list(range(101, 121))\n",
        "\n",
        "BATCH_SIZE = 20   # Number of episodes to process at one time\n",
        "COMPARE_EPISODES = min(5, len(EPISODE_NUMBERS))\n",
        "\n",
        "# Minimum number of sentences required in an episode\n",
        "MIN_SENTENCES = 10                          # lowered for testing\n",
        "\n",
        "# Filename patterns\n",
        "def input_filename(num):\n",
        "    return f\"episode_{num}_whisper.json\"\n",
        "\n",
        "def output_filename(num):\n",
        "    return f\"episode_{num}_segment.json\"\n",
        "\n",
        "# Build & validate file list\n",
        "files_to_process = []\n",
        "for num in EPISODE_NUMBERS:\n",
        "    fname = input_filename(num)\n",
        "    full_path = os.path.join(PROCESSED_DIR, fname)\n",
        "    if os.path.exists(full_path):\n",
        "        files_to_process.append((num, fname))\n",
        "    else:\n",
        "        print(f\"Missing: {fname}\")\n",
        "\n",
        "print(f\"\\nWill process {len(files_to_process)} episodes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLoRip9kvKHP",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1768559228582,
          "user_tz": -330,
          "elapsed": 24,
          "user": {
            "displayName": "Manasi N",
            "userId": "07680321668724852566"
          }
        },
        "outputId": "caf59d6f-74d8-4145-c3c3-788df01485d4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Will process 20 episodes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODELS**"
      ],
      "metadata": {
        "id": "aU8hvTytl8ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading models...\")\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "device = 0 if torch.cuda.is_available() else -1   # Check if a GPU is available\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=device)\n",
        "\n",
        "STOP_WORDS = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5vpIYACl_ak",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1768559237977,
          "user_tz": -330,
          "elapsed": 6034,
          "user": {
            "displayName": "Manasi N",
            "userId": "07680321668724852566"
          }
        },
        "outputId": "a9d3928d-e9f4-45c6-f96e-c6db7a79b27a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREPARE TRANSCRIPT + SPLIT INTO SENTENCES**"
      ],
      "metadata": {
        "id": "0x8HY4aamL7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_episode(num, filename):\n",
        "    path = os.path.join(PROCESSED_DIR, filename)\n",
        "    with open(path, 'r', encoding='utf-8') as f:    # Open and load the JSON file\n",
        "        data = json.load(f)\n",
        "\n",
        "    text_parts = []   # This list will store all text pieces found in the file\n",
        "\n",
        "    # Case 1: Standard Whisper output with 'segments'\n",
        "    if isinstance(data, dict) and 'segments' in data:\n",
        "        for seg in data.get('segments', []):\n",
        "            text = seg.get('text', '').strip()\n",
        "            if text:\n",
        "                text_parts.append(text)\n",
        "\n",
        "    # Case 2: List of dicts\n",
        "    elif isinstance(data, list):\n",
        "        for item in data:\n",
        "            text = item.get('text') or item.get('line') or item.get('content') or \"\"\n",
        "            if text.strip():\n",
        "                text_parts.append(text.strip())\n",
        "\n",
        "    # Case 3: Single string under common keys\n",
        "    elif isinstance(data, dict):\n",
        "        for k in ['text', 'transcript', 'full_text', 'transcription']:\n",
        "            if k in data and isinstance(data[k], str):\n",
        "                text_parts.append(data[k].strip())\n",
        "                break\n",
        "\n",
        "    # Fallback\n",
        "    if not text_parts:\n",
        "        text_parts = [str(data)[:2000]]\n",
        "\n",
        "    full_text = \" \".join(text_parts).strip()\n",
        "    sentences = sent_tokenize(full_text)\n",
        "\n",
        "    # Timestamps (if available in segments)\n",
        "    timestamps = [0.0] * len(sentences)\n",
        "    if isinstance(data, dict) and 'segments' in data:\n",
        "        seg_idx = 0\n",
        "        sent_idx = 0\n",
        "        while sent_idx < len(sentences) and seg_idx < len(data['segments']):\n",
        "            seg = data['segments'][seg_idx]\n",
        "            ts = seg.get('start') or seg.get('timestamp') or 0.0\n",
        "            seg_text = seg.get('text', '')\n",
        "            seg_sent_count = len(sent_tokenize(seg_text))\n",
        "            for _ in range(min(seg_sent_count, len(sentences) - sent_idx)):\n",
        "                timestamps[sent_idx] = ts\n",
        "                sent_idx += 1\n",
        "            seg_idx += 1\n",
        "\n",
        "    ep_id = f\"episode_{num}\"\n",
        "    print(f\"  Loaded {ep_id:20} | {len(sentences):4,} sentences | text len: {len(full_text):,}\")\n",
        "\n",
        "    return ep_id, sentences, timestamps, full_text"
      ],
      "metadata": {
        "id": "rnJ6UffImNU1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1768559252950,
          "user_tz": -330,
          "elapsed": 42,
          "user": {
            "displayName": "Manasi N",
            "userId": "07680321668724852566"
          }
        }
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ALGORITHM 1 - BASELINE (SENTENCE SIMILARITY)**"
      ],
      "metadata": {
        "id": "RG4dL6qSmSBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Algorithm 1 \u2013 Baseline segmentation\n",
        "This method splits text when two nearby sentences are not very similar.\n",
        "\"\"\"\n",
        "def segment_baseline(sentences, threshold=0.23):\n",
        "    if len(sentences) < MIN_SENTENCES:\n",
        "        return [\" \".join(sentences)], [0, len(sentences)]\n",
        "\n",
        "    vec = TfidfVectorizer(stop_words='english')   # Convert sentences into numerical vectors using TF-IDF\n",
        "    X = vec.fit_transform(sentences)\n",
        "\n",
        "    boundaries = [0]    # Store the starting index of each segment\n",
        "\n",
        "    # Compare each sentence with the previous one\n",
        "    for i in range(1, len(sentences)):\n",
        "        sim = cosine_similarity(X[i-1:i], X[i:i+1])[0][0]   # Measure how similar the two sentences are\n",
        "\n",
        "        # If similarity is low, start a new segment\n",
        "        if sim < threshold:\n",
        "            boundaries.append(i)    # Add the end of the last segment\n",
        "    boundaries.append(len(sentences))\n",
        "\n",
        "    # Combine sentences into text segments using the boundaries\n",
        "    segments = [\" \".join(sentences[s:e]) for s, e in zip(boundaries[:-1], boundaries[1:])]\n",
        "    return segments, boundaries"
      ],
      "metadata": {
        "id": "GhA9pjGtmSpd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1768559256490,
          "user_tz": -330,
          "elapsed": 7,
          "user": {
            "displayName": "Manasi N",
            "userId": "07680321668724852566"
          }
        }
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ALGORITHM 2 - EMBEDDING-BASED**"
      ],
      "metadata": {
        "id": "-hzN8dq-mU7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Algorithm 2 \u2013 Embedding-based segmentation (final chosen method)\n",
        "This method splits text based on meaning similarity between sentence groups.\n",
        "\"\"\"\n",
        "def segment_embedding(sentences, window=5, threshold=0.48):\n",
        "\n",
        "    # If there are too few sentences, return everything as one segment\n",
        "    if len(sentences) < MIN_SENTENCES:\n",
        "        return [\" \".join(sentences)], [0, len(sentences)]\n",
        "\n",
        "    # Convert each sentence into an embedding (numerical meaning vector)\n",
        "    embs = embed_model.encode(sentences, show_progress_bar=False, batch_size=32)\n",
        "\n",
        "    boundaries = [0]\n",
        "\n",
        "    # Move a sliding window across the embeddings\n",
        "    for i in range(window, len(embs)-window):\n",
        "        left  = embs[i-window:i].mean(axis=0)   # Average embeddings for sentences before the current point\n",
        "        right = embs[i:i+window].mean(axis=0)   # Average embeddings for sentences after the current point\n",
        "        sim = cosine_similarity([left], [right])[0][0]\n",
        "        if sim < threshold:\n",
        "            boundaries.append(i)\n",
        "    boundaries.append(len(embs))\n",
        "\n",
        "    segments = [\" \".join(sentences[s:e]) for s, e in zip(boundaries[:-1], boundaries[1:])]\n",
        "    return segments, boundaries"
      ],
      "metadata": {
        "id": "gUJRXxD8mXJO",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1768559259898,
          "user_tz": -330,
          "elapsed": 8,
          "user": {
            "displayName": "Manasi N",
            "userId": "07680321668724852566"
          }
        }
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ALGORITHM 3 - LLM**"
      ],
      "metadata": {
        "id": "afd072K74f5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_llm_openrouter(full_text):\n",
        "    # Check if the OpenRouter LLM client is available or not\n",
        "    if not llm_client:\n",
        "        print(\"OpenRouter client not initialized \u2013 skipping LLM\")\n",
        "        return [0]\n",
        "\n",
        "    # System message that tells the LLM exactly what to do\n",
        "    system = \"\"\"You are an expert podcast topic segmenter.\n",
        "Given a transcript chunk, return ONLY a Python list of sentence indices (starting from 0) where a NEW topic begins.\n",
        "Example: [0, 42, 118, 195]\n",
        "No explanation, no extra text.\"\"\"\n",
        "\n",
        "    # User message containing the transcript text\n",
        "    user = f\"\"\"Transcript:\n",
        "{full_text[:90000]}   # Text is limited to the first 90,000 characters to avoid token limits\n",
        "\n",
        "Return only the list of boundary indices.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Send the request to OpenRouter using a free LLM model\n",
        "        resp = llm_client.chat.completions.create(\n",
        "            model=\"xiaomi/mimo-v2-flash:free\",   # working free model (Jan 16, 2026)\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system},\n",
        "                {\"role\": \"user\", \"content\": user}\n",
        "            ],\n",
        "            temperature=0.1,    # Low temperature for stable, consistent output\n",
        "            max_tokens=300,     # Enough tokens for a short list of numbers\n",
        "        )\n",
        "\n",
        "        # Extract the raw text returned by the model\n",
        "        raw = resp.choices[0].message.content.strip()\n",
        "\n",
        "        # Safely convert the returned string into a Python object\n",
        "        import ast\n",
        "        try:\n",
        "            bounds = ast.literal_eval(raw)\n",
        "\n",
        "            # Make sure the result is a list of integers\n",
        "            if isinstance(bounds, list) and all(isinstance(x, int) for x in bounds):\n",
        "                return bounds\n",
        "        except:\n",
        "            pass\n",
        "        return [0]    # If parsing fails, return a default boundary\n",
        "    except Exception as e:\n",
        "        print(f\"OpenRouter error: {e}\")\n",
        "        return [0]"
      ],
      "metadata": {
        "id": "p2pCMr3z4md4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1768559262556,
          "user_tz": -330,
          "elapsed": 6,
          "user": {
            "displayName": "Manasi N",
            "userId": "07680321668724852566"
          }
        }
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEST OPENROUTER CONNECTION**"
      ],
      "metadata": {
        "id": "QXAoiMDuZdv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test OpenRouter connection & segmentation\n",
        "test_text = \"This is a test transcript. First part about weather. Second part about AI.\"\n",
        "bounds = segment_llm_openrouter(test_text)\n",
        "print(\"Test boundaries:\", bounds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKtCyAERUue3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1768559267268,
          "user_tz": -330,
          "elapsed": 1443,
          "user": {
            "displayName": "Manasi N",
            "userId": "07680321668724852566"
          }
        },
        "outputId": "6a76fc5f-475f-4c67-9371-93d17205847e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test boundaries: [0, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF KEYWORDS SEGMENT**"
      ],
      "metadata": {
        "id": "b89R18kIma5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_keywords(text, top_n=6):\n",
        "    # If the text is very short, keyword extraction is not useful\n",
        "    if len(text.split()) < 20:\n",
        "        return []\n",
        "\n",
        "    # Create a TF-IDF vectorizer\n",
        "    vec = TfidfVectorizer(stop_words=list(STOP_WORDS), max_features=250)\n",
        "    try:\n",
        "        X = vec.fit_transform([text])           # Convert the text into TF-IDF scores\n",
        "        names = vec.get_feature_names_out()     # Get the list of words used by the vectorizer\n",
        "        scores = X.toarray()[0]                 # Get the importance score for each word\n",
        "        idx = scores.argsort()[-top_n:][::-1]   # Find the indices of the top N highest-scoring words\n",
        "        return [names[i] for i in idx]          # Return the top keywords\n",
        "    except:\n",
        "        return []   # If anything goes wrong, return an empty list"
      ],
      "metadata": {
        "id": "JCTa82DYmb2M",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1768559269722,
          "user_tz": -330,
          "elapsed": 7,
          "user": {
            "displayName": "Manasi N",
            "userId": "07680321668724852566"
          }
        }
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SHORT SUMMARIES (1\u20132 SENTENCES)**"
      ],
      "metadata": {
        "id": "rRe17iYUmfT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_summary(text):\n",
        "    # If the text is very short, return a shortened preview instead of summarizing\n",
        "    if len(text.split()) < 50:\n",
        "        return text[:220] + \" \u2026\"\n",
        "    input_len = len(text.split())\n",
        "\n",
        "    # Decide the maximum length of the summary\n",
        "    max_len = min(65, max(30, int(input_len * 0.6)))\n",
        "    try:\n",
        "        # Generate a summary using the summarization model\n",
        "        return summarizer(text, max_length=max_len, min_length=max(15, max_len//2),\n",
        "                          do_sample=False, truncation=True)[0]['summary_text']\n",
        "    except:\n",
        "        # If summarization fails, return a short preview instead\n",
        "        return text[:220] + \" \u2026\""
      ],
      "metadata": {
        "id": "EMZU6DfomgAV",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1768559272859,
          "user_tz": -330,
          "elapsed": 42,
          "user": {
            "displayName": "Manasi N",
            "userId": "07680321668724852566"
          }
        }
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAIN PROCESSING**"
      ],
      "metadata": {
        "id": "n3YMNOWGmh7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Processing \u2013 {len(files_to_process)} episodes\")\n",
        "total_start = time.time()   # Record the overall start time\n",
        "\n",
        "comparison_rows = []    # This list will store comparison results for analysis\n",
        "\n",
        "# Process episodes in batches\n",
        "for batch_idx, start in enumerate(range(0, len(files_to_process), BATCH_SIZE)):\n",
        "    batch = files_to_process[start : start + BATCH_SIZE]\n",
        "    print(f\"\\nBatch {batch_idx+1} ({len(batch)} episodes)\")\n",
        "\n",
        "    batch_start_time = time.time()\n",
        "\n",
        "    # Loop through each episode in the batch with a progress bar\n",
        "    for ep_num, fname in tqdm(batch, desc=f\"Batch {batch_idx+1}\"):\n",
        "        ep_start = time.time()\n",
        "\n",
        "        # Loop through each episode in the batch with a progress bar\n",
        "        ep_id, sentences, timestamps, full_text = load_episode(ep_num, fname)\n",
        "\n",
        "        # Skip episodes that are too short\n",
        "        if len(sentences) < MIN_SENTENCES:\n",
        "            print(f\"  Skipping {ep_id} \u2013 too short ({len(sentences)} sentences)\")\n",
        "            continue\n",
        "\n",
        "        # ---- Baseline segmentation (TF-IDF similarity) ----\n",
        "        segs_base, _ = segment_baseline(sentences)\n",
        "\n",
        "        # ---- Embedding-based segmentation (final method) ----\n",
        "        segs_emb, bounds_emb = segment_embedding(sentences)\n",
        "\n",
        "        # ---- LLM-based segmentation ----\n",
        "        llm_segments_count = 0\n",
        "        if len(comparison_rows) < COMPARE_EPISODES:\n",
        "            llm_bounds = segment_llm_openrouter(full_text)\n",
        "            llm_segments_count = len(llm_bounds) - 1 if llm_bounds else 0\n",
        "\n",
        "        # Comparison data\n",
        "        comparison_rows.append({\n",
        "            \"episode\": ep_id,\n",
        "            \"sentences\": len(sentences),\n",
        "            \"baseline_segments\": len(segs_base),\n",
        "            \"embedding_segments\": len(segs_emb),\n",
        "            \"llm_segments\": llm_segments_count,\n",
        "            \"time_sec\": round(time.time() - ep_start, 1)\n",
        "        })\n",
        "\n",
        "        # Final output \u2013 embedding method\n",
        "        output = {\n",
        "            \"episode_id\": ep_id,\n",
        "            \"source_file\": fname,\n",
        "            \"total_sentences\": len(sentences),\n",
        "            \"algorithm_used\": \"embedding-based (Sentence Transformers)\",\n",
        "            \"segments\": []\n",
        "        }\n",
        "\n",
        "        # Build each segment using the computed boundaries\n",
        "        for i, (s, e) in enumerate(zip(bounds_emb[:-1], bounds_emb[1:]), 1):\n",
        "            # Combine sentences for this segment\n",
        "            seg_text = \" \".join(sentences[s:e])\n",
        "\n",
        "            # Get start and end timestamps (if available)\n",
        "            start_t = timestamps[s] if s < len(timestamps) else 0.0\n",
        "            end_t   = timestamps[e-1] if e-1 < len(timestamps) else start_t\n",
        "\n",
        "            # Add segment details to output\n",
        "            output[\"segments\"].append({\n",
        "                \"segment_id\": i,\n",
        "                \"start_sentence_idx\": s,\n",
        "                \"end_sentence_idx\": e,\n",
        "                \"num_sentences\": e - s,\n",
        "                \"start_time_sec\": round(float(start_t), 2),\n",
        "                \"end_time_sec\": round(float(end_t), 2),\n",
        "                \"text_preview\": seg_text[:280] + \"\u2026\" if len(seg_text) > 280 else seg_text,\n",
        "                \"keywords\": get_keywords(seg_text),\n",
        "                \"summary\": get_summary(seg_text)\n",
        "            })\n",
        "\n",
        "        # Save the final segmented output as a JSON file\n",
        "        out_path = os.path.join(OUTPUT_DIR, output_filename(ep_num))\n",
        "        with open(out_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(output, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Print timing and segmentation summary for this episode\n",
        "        duration = time.time() - ep_start\n",
        "        print(f\"  {ep_id:20} | emb: {len(segs_emb):2d} seg | llm: {llm_segments_count:2d} | {duration:.1f} s\")\n",
        "\n",
        "    # Print how long the batch took to process\n",
        "    batch_duration = time.time() - batch_start_time\n",
        "    print(f\"Batch {batch_idx+1} finished in {batch_duration/60:.1f} min\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867,
          "referenced_widgets": [
            "e529af7f03df4579a79887a224619a17",
            "23fa22fae0e94023a5baa830b24fdacf",
            "f5a553f091a04a7690f1b91af3cfbf11",
            "a5b1c17a2de1432080a293f6b64bc338",
            "70522d72f5844724953763114b1b6eac",
            "b429e05f6c7249bbb353d41ff4bbcd88",
            "fbecb6cb3cf644609094a066b4bb6d3b",
            "fabb7e7316e043b593ca31804d1e1a34",
            "c830da4547a44e8bb114dae8ee727056",
            "25e9cada36f54f4f9b45cf6432aa8114",
            "1a09a10a3bbd4822ba97bf9a8cbe0627"
          ]
        },
        "collapsed": true,
        "id": "uB-Rx8mWmm9-",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1768559808452,
          "user_tz": -330,
          "elapsed": 533462,
          "user": {
            "displayName": "Manasi N",
            "userId": "07680321668724852566"
          }
        },
        "outputId": "ad5fa064-fa53-4603-fc4f-5d7e469ed5d4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing \u2013 20 episodes\n",
            "\n",
            "Batch 1 (20 episodes)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batch 1:   0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e529af7f03df4579a79887a224619a17"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded episode_101          |  763 sentences | text len: 48,137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  episode_101          | emb: 553 seg | llm:  0 | 53.0 s\n",
            "  Loaded episode_102          |  799 sentences | text len: 48,645\n",
            "  episode_102          | emb: 476 seg | llm:  0 | 34.3 s\n",
            "  Loaded episode_103          |  798 sentences | text len: 44,470\n",
            "  episode_103          | emb: 487 seg | llm:  0 | 29.2 s\n",
            "  Loaded episode_104          |  629 sentences | text len: 44,175\n",
            "  episode_104          | emb: 445 seg | llm:  0 | 35.2 s\n",
            "  Loaded episode_105          |  790 sentences | text len: 46,696\n",
            "  episode_105          | emb: 432 seg | llm:  0 | 33.4 s\n",
            "  Loaded episode_106          |  829 sentences | text len: 45,264\n",
            "  episode_106          | emb: 492 seg | llm:  0 | 32.6 s\n",
            "  Loaded episode_107          |  604 sentences | text len: 43,272\n",
            "  episode_107          | emb: 389 seg | llm:  0 | 20.6 s\n",
            "  Loaded episode_108          |  739 sentences | text len: 43,275\n",
            "  episode_108          | emb: 437 seg | llm:  0 | 26.0 s\n",
            "  Loaded episode_109          |  879 sentences | text len: 43,488\n",
            "  episode_109          | emb: 573 seg | llm:  0 | 14.6 s\n",
            "  Loaded episode_110          |  676 sentences | text len: 39,933\n",
            "  episode_110          | emb: 440 seg | llm:  0 | 27.5 s\n",
            "  Loaded episode_111          |  751 sentences | text len: 40,397\n",
            "  episode_111          | emb: 495 seg | llm:  0 | 15.8 s\n",
            "  Loaded episode_112          |  688 sentences | text len: 50,082\n",
            "  episode_112          | emb: 454 seg | llm:  0 | 24.6 s\n",
            "  Loaded episode_113          |  787 sentences | text len: 48,798\n",
            "  episode_113          | emb: 566 seg | llm:  0 | 19.2 s\n",
            "  Loaded episode_114          |  666 sentences | text len: 42,776\n",
            "  episode_114          | emb: 334 seg | llm:  0 | 22.6 s\n",
            "  Loaded episode_115          |  782 sentences | text len: 46,340\n",
            "  episode_115          | emb: 531 seg | llm:  0 | 19.6 s\n",
            "  Loaded episode_116          |  683 sentences | text len: 40,082\n",
            "  episode_116          | emb: 412 seg | llm:  0 | 16.9 s\n",
            "  Loaded episode_117          |  737 sentences | text len: 43,571\n",
            "  episode_117          | emb: 428 seg | llm:  0 | 29.7 s\n",
            "  Loaded episode_118          |  636 sentences | text len: 44,052\n",
            "  episode_118          | emb: 452 seg | llm:  0 | 26.0 s\n",
            "  Loaded episode_119          |  695 sentences | text len: 47,921\n",
            "  episode_119          | emb: 450 seg | llm:  0 | 28.8 s\n",
            "  Loaded episode_120          |  856 sentences | text len: 44,093\n",
            "  episode_120          | emb: 589 seg | llm:  0 | 23.6 s\n",
            "Batch 1 finished in 8.9 min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE COMPARISON FILE**"
      ],
      "metadata": {
        "id": "QkaOBHgX5G6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if there is any comparison data to save\n",
        "if comparison_rows:\n",
        "    # Convert the comparison results into a DataFrame\n",
        "    df_compare = pd.DataFrame(comparison_rows)\n",
        "\n",
        "    # Create the output file path for the comparison results\n",
        "    compare_path = os.path.join(COMPARE_DIR, \"segmentation_algorithm_comparison.json\")\n",
        "\n",
        "    # Save the comparison data as a JSON file\n",
        "    df_compare.to_json(compare_path, orient=\"records\", indent=2)\n",
        "    print(f\"\\nComparison saved \u2192 {compare_path}\")\n",
        "    print(df_compare.to_string(index=False))\n",
        "else:\n",
        "    print(\"\\nNo episodes were processed \u2192 no comparison file created\")\n",
        "\n",
        "# Calculate total processing time\n",
        "total_duration = time.time() - total_start\n",
        "print(f\"\\nTotal time: {total_duration/60:.1f} minutes\")   # Print total runtime in minutes\n",
        "print(\"Output folder:\", COMPARE_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLgsIm585Jgv",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1768559827306,
          "user_tz": -330,
          "elapsed": 147,
          "user": {
            "displayName": "Manasi N",
            "userId": "07680321668724852566"
          }
        },
        "outputId": "aedf92cb-a558-4279-b74f-15deb247c812"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparison saved \u2192 /content/drive/MyDrive/podcast-project/data/segmented_outputs/algorithm_comparison/segmentation_algorithm_comparison.json\n",
            "    episode  sentences  baseline_segments  embedding_segments  llm_segments  time_sec\n",
            "episode_101        763                705                 553             0       8.0\n",
            "episode_102        799                728                 476             0       7.9\n",
            "episode_103        798                727                 487             0      10.8\n",
            "episode_104        629                605                 445             0      11.7\n",
            "episode_105        790                723                 432             0       8.3\n",
            "episode_106        829                735                 492             0       1.6\n",
            "episode_107        604                571                 389             0       1.2\n",
            "episode_108        739                664                 437             0       1.4\n",
            "episode_109        879                786                 573             0       1.6\n",
            "episode_110        676                605                 440             0       1.3\n",
            "episode_111        751                696                 495             0       1.3\n",
            "episode_112        688                640                 454             0       1.4\n",
            "episode_113        787                731                 566             0       1.4\n",
            "episode_114        666                627                 334             0       1.5\n",
            "episode_115        782                724                 531             0       1.4\n",
            "episode_116        683                630                 412             0       1.5\n",
            "episode_117        737                688                 428             0       1.4\n",
            "episode_118        636                587                 452             0       2.1\n",
            "episode_119        695                651                 450             0       1.8\n",
            "episode_120        856                764                 589             0       2.1\n",
            "\n",
            "Total time: 9.2 minutes\n",
            "Output folder: /content/drive/MyDrive/podcast-project/data/segmented_outputs/algorithm_comparison\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1NCdchrUpGymZuTMnO_5bHzS13GVKy17W",
      "authorship_tag": "ABX9TyNGMUJfl6wWLLXMQ5RA10Qu"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}