# Data Directory

This directory contains all data assets for the Automated Podcast Transcription and Segmentation project, including raw audio files, transcripts, and processed data.

## Directory Structure

### `audio_raw/`
**Purpose**: Storage location for raw, unprocessed podcast audio files.

- **Contents**: Original MP3 files downloaded from the Kaggle dataset
- **Size**: Large files
- **Source**: [This American Life Episodes](https://www.thisamericanlife.org/archive)
- **Use Case**: Input source for audio preprocessing and speech-to-text conversion

---

### `audio_processed/`
**Purpose**: Stores preprocessed and normalized audio files after initial cleaning.

- **Contents**: Cleaned WAV files with noise reduction and loudness normalization
- **Format**: Mono, 16 kHz sample rate, 16-bit WAV
- **Processing**: Noise reduction, loudness normalization to -14 LUFS (optimal for podcasts)
- **Use Case**: Intermediate format for speech-to-text processing

---

### `audio_tmp/`
**Purpose**: Temporary storage for audio chunks during processing pipelines.

- **Contents**: Episode-specific subdirectories containing chunked audio segments
- **Format**: 30-second audio chunks in WAV format
- **Structure**: 
  ```
  audio_tmp/
  ├── episode_1/
  │   ├── conditioned.wav
  │   ├── chunk_0.wav
  │   ├── chunk_1.wav
  │   └── ...
  ├── episode_2/
  └── ...
  ```
- **Lifecycle**: Generated during preprocessing, can be safely deleted after transcription
- **Use Case**: Divide long audio files into manageable chunks for ASR processing

---

### `transcripts_raw/`
**Purpose**: Original reference transcripts downloaded from the Kaggle dataset.

- **Contents**:
  - `lines_clean.csv` - Individual transcript lines with metadata
  - `episode_info_clean.csv` - Episode-level metadata
- **Format**: CSV (Comma-Separated Values)
- **Rows**: Full dataset (~600 episodes)
- **Columns** (lines_clean.csv):
  - `episode_id`: Unique episode identifier
  - `line_text`: Spoken text from the transcript
  - Additional metadata fields
- **Source**: [This American Life Podcast Transcript Dataset](https://www.kaggle.com/datasets/thedevastator/this-american-life-podcast-transcript-dataset)
- **Use Case**: Ground truth reference for transcript quality evaluation and WER (Word Error Rate) calculation

---

### `transcripts_raw_truncated/`
**Purpose**: Sample subset of raw transcripts for development and testing.

- **Contents**:
  - `lines_clean_200.csv` - First 200 rows of line transcripts
  - `episode_info_clean_200.csv` - Corresponding episode metadata
- **Format**: CSV
- **Rows**: 200 sample rows (subset for faster iteration)
- **Status**: Tracked in Git
- **Use Case**: 
  - Development and testing without processing full dataset
  - Quick prototyping and validation
  - Baseline quality evaluation

---

### `transcripts_processed/`
**Purpose**: Transcripts generated by OpenAI Whisper (automatic speech recognition).

- **Contents**: JSON files with Whisper-generated transcriptions
- **File Format**: `episode_{N}_whisper.json`
- **Structure**:
  ```json
  {
    "text": "Full transcribed text from Whisper...",
    "segments": [...],
    "language": "en"
  }
  ```
- **Status**: Tracked in Git
- **Generation**: Created by running audio through Whisper ASR model
- **Use Case**: 
  - Primary output of the speech-to-text pipeline
  - Evaluation against ground truth transcripts
  - Input for downstream NLP tasks

---

## Data Pipeline Flow

```
audio_raw/
    ↓ (Preprocessing: noise reduction, normalization)
audio_processed/
    ↓ (Chunking: 30-second segments)
audio_tmp/ (temporary chunks)
    ↓ (Speech-to-Text: Whisper ASR)
transcripts_processed/ (JSON outputs)
    ↓ (Quality Evaluation: Compare against transcripts_raw/)
Quality Metrics (WER, accuracy)
```

---

## Dataset Information

| Metric | Value |
|--------|-------|
| **Dataset Source** | Kaggle - This American Life |
| **Number of Episodes** | 200 |
| **Total Audio Size** | ~6 GB |
| **Audio Format** | MP3 (original), WAV (processed) |
| **Sample Rate** | 16 kHz |
| **Transcript Format** | CSV (raw), JSON (processed) |

---

## Important Notes

- **Large Files**: Audio files are large and excluded from version control
- **Processing Time**: Full audio processing requires significant compute resources (GPU recommended)
- **Temporary Files**: `audio_tmp/` can be safely deleted and regenerated
- **Quality Metrics**: Compare Whisper transcripts against `transcripts_raw/` for accuracy evaluation
- **Normalization**: All audio is normalized to -14 LUFS for consistent loudness (podcast standard)

---

## Getting Started

1. Obtain raw audio files from the Kaggle dataset and place in `audio_raw/`
2. Run preprocessing pipeline to generate `audio_processed/` files
3. Execute chunking to create `audio_tmp/` segments
4. Run Whisper ASR to generate `transcripts_processed/` JSON files
5. Evaluate quality using metrics against `transcripts_raw/`

For detailed processing instructions, see the notebooks in `notebooks/milestone_1/`
