IBM pulled in to the Granite 4H series models, right?
So composition here, you've got a 9 to 1 ratio of Mamba layers to attention layers and the Granite 4H families.
So quite a bit of state space model in that hybrid.
Those are, like I said earlier, those are linear context scaling.
They are the Granite 4H.
