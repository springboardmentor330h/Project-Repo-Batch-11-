
Segment 1 Summary:
IBM pulled in to the Granite 4H series models, right?

Segment 2 Summary:
So composition here, you've got a 9 to 1 ratio of Mamba layers to attention layers and the Granite 4H families 

Segment 3 Summary:
So quite a bit of state space model in that hybrid 

Segment 4 Summary:
Those are, like I said earlier, those are linear context scaling 

Segment 5 Summary:
They are the Granite 4H 
