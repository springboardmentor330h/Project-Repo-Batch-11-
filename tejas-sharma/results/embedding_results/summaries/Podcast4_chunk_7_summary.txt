
Segment 1 Summary:
And then you go into structured state-space models with selection and computation by scanning your S6, which turns into, hey, that's a lot of S's, sounds like a snake 

Segment 2 Summary:
Now we've got Mamba 

Segment 3 Summary:
And the Mamba architecture really optimizes state-space models for the kind of compute profile that's needed to be relevant for the language model tasks that we're applying here 
