
Segment 1 Summary:
sessions at 128k context on a micro, so a 3 billion parameter model, you get about a 15 gigabyte of memory usage versus about 80 on a pure transformer architecture 

Segment 2 Summary:
Right 

Segment 3 Summary:
So some really great context reduction, which means that on more constrained devices on the edge, you can make use of more useful context 
