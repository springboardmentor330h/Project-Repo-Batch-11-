
--- Segment 1 ---
IBM pulled in to the Granite 4H series models, right?

--- Segment 2 ---
So composition here, you've got a 9 to 1 ratio of Mamba layers to attention layers and the Granite 4H families.

--- Segment 3 ---
So quite a bit of state space model in that hybrid.

--- Segment 4 ---
Those are, like I said earlier, those are linear context scaling.

--- Segment 5 ---
They are the Granite 4H.
